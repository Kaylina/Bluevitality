### multiprocessing

```python
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print 'Run task %s (%s)...' % (name, os.getpid())
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print 'Task %s runs %0.2f seconds.' % (name, (end - start))

if __name__=='__main__':
    print 'Parent process %s.' % os.getpid()
    p = Pool()        #Pool的默认大小是CPU的核数
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print 'Waiting for all subprocesses done...'
    p.close()
    p.join()
    print 'All subprocesses done.'

输出：
Parent process 669.
Waiting for all subprocesses done...
Run task 0 (671)...
Run task 1 (672)...
Run task 2 (673)...
Run task 3 (674)...
Task 2 runs 0.14 seconds.
Run task 4 (673)...
Task 1 runs 0.27 seconds.
Task 3 runs 0.86 seconds.
Task 0 runs 1.41 seconds.
Task 4 runs 1.91 seconds.
All subprocesses done.
```

* 在利用Python进行系统管理时，特别是同时操作多个文件目录或远程控制多台主机，并行操作可节约大量时间。当被操作对象数目不大时可直接利用multiprocessing中的Process动态成生多个进程，十几个还好，但如果是上百个，上千个目标，手动去限制进程数量却又太过繁琐，此时可以发挥进程池的功效。
* Pool可提供指定数量的进程供用户调用，当有新的请求提交到pool中时，若池还未满就会创建新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束才会创建新的进程。
```python
#coding: utf-8
import multiprocessing
import time

def func(msg):
    print "msg:", msg
    time.sleep(3)
    print "end"

if __name__ == "__main__":
    pool = multiprocessing.Pool(processes = 3)      #指定进程池数量
    for i in xrange(4):                             #指定任务数量
        msg = "hello %d" %(i)
        pool.apply_async(func, (msg, ))             #维持并发数为processes，当1个进程执行完毕再启动新的进程

    print "——————————————————————"
    pool.close()
    pool.join()  #调用join前先调close否则会出错。执行完close后不会有新的进程加入pool,join用于等待所有子进程结束
    print "Sub-process(es) done."

输出：
mMsg: ——————————————————————
 
msg: hello 1
msg: hello 2
end
msg: hello 3
end
end
end
Sub-process(es) done.

"""
说明：
创建一个进程池pool，并设定进程的数量为3，xrange(4)会相继产生四个对象[0, 1, 2, 4]，四个对象被提交到pool中，因pool指定进程数为3，所以0、1、2会直接送到进程中执行，当其中一个执行完事后才空出一个进程处理对象3，所以会出现输出“msg: hello 3”出现在"end"后。因为为非阻塞，主函数会自己执行自个的，不搭理进程的执行，所以运行完for循环后直接输出“mMsg: hark~ Mark~ Mark~~~~~~~~~~~~~~~~~~~~~~”，主程序在pool.join（）处等待各个进程的结束
"""
```

####  函数说明
- 非阻塞  `apply_async(func[, args[, kwds[, callback]]])` **从池中取出1个进程执行func，args为func的参数。它将返回一个AsyncResult的对象，可以对该对象调用get()方法以获得结果**
- 阻塞    `apply(func[, args[, kwds]])`
- 关闭pool，使其不再接受新的进程`close()`**进程池不再创建新的进程**
- 结束子进程且不再处理未完成任务`terminate()`
- 主进程阻塞住以等待子进程的退出`join()`  **join()要在close或terminate之后使用**
- Join()等待子进程结束后再继续往下运行，通常用于进程间的同步

#### 阻塞方式调用子进程
```python
#coding: utf-8
import multiprocessing
import time

def func(msg):
    print "msg:", msg
    time.sleep(3)
    print "end"

if __name__ == "__main__":
    pool = multiprocessing.Pool(processes = 3)
    for i in xrange(4):
        msg = "hello %d" %(i)
        pool.apply(func, (msg, ))       #维持并发数为processes，当1个进程执行完毕后会启动新的进程

    print "Mark~ Mark~ Mark~~~~~~~~~~~~~~~~~~~~~~"
    pool.close()
    pool.join()  #调用join前先调用close否则会出错。执行完close后不会有新的进程加入pool,join等待所有子进程结束
    print "Sub-process(es) done."
```

#### 使用进程池
```python
#coding: utf-8
import multiprocessing
import time

def func(msg):
    print "msg:", msg
    time.sleep(3)
    print "end"
    return "done" + msg

if __name__ == "__main__":
    pool = multiprocessing.Pool(processes=4)
    result = []
    for i in xrange(3):
        msg = "hello %d" %(i)
        result.append(pool.apply_async(func, (msg, )))
    pool.close()
    pool.join()
    for res in result:
        print ":::", res.get()          #get函数得出每个返回结果的值
    print "Sub-process(es) done."
    
输出：
msg: hello 0
msg: hello 1
msg: hello 2
end
end
end
::: donehello 0
::: donehello 1
::: donehello 2
Sub-process(es) done.
```

#### 多个进程池
```python
#coding: utf-8
import multiprocessing
import os, time, random

def Lee():
    print "\nRun task Lee-%s" %(os.getpid()) #os.getpid()获取当前的进程的ID
    start = time.time()
    time.sleep(random.random() * 10) #random.random()随机生成0-1之间的小数
    end = time.time()
    print 'Task Lee, runs %0.2f seconds.' %(end - start)

def Marlon():
    print "\nRun task Marlon-%s" %(os.getpid())
    start = time.time()
    time.sleep(random.random() * 40)
    end=time.time()
    print 'Task Marlon runs %0.2f seconds.' %(end - start)

def Allen():
    print "\nRun task Allen-%s" %(os.getpid())
    start = time.time()
    time.sleep(random.random() * 30)
    end = time.time()
    print 'Task Allen runs %0.2f seconds.' %(end - start)

def Frank():
    print "\nRun task Frank-%s" %(os.getpid())
    start = time.time()
    time.sleep(random.random() * 20)
    end = time.time()
    print 'Task Frank runs %0.2f seconds.' %(end - start)
        
if __name__=='__main__':
    function_list=  [Lee, Marlon, Allen, Frank] 
    print "parent process %s" %(os.getpid())

    pool=multiprocessing.Pool(4)
    for func in function_list:
        pool.apply_async(func)     #Pool执行函数，apply执行函数,当有一个进程执行完毕后，会添加一个新的进程到pool中

    print 'Waiting for all subprocesses done...'
    pool.close()
    pool.join()    #调用join之前，一定要先调用close() 函数，否则会出错, close()执行后不会有新的进程加入到pool,join函数等待素有子进程结束
    print 'All subprocesses done.'

输出：
parent process 7704
Waiting for all subprocesses done...
Run task Lee-6948
Run task Marlon-2896
Run task Allen-7304
Run task Frank-3052
Task Lee, runs 1.59 seconds.
Task Marlon runs 8.48 seconds.
Task Frank runs 15.68 seconds.
Task Allen runs 18.08 seconds.
All subprocesses done.
```

#### Pool Map
```python
#coding: utf-8
import multiprocessing 

def m1(x): 
    print x * x 

if __name__ == '__main__': 
    pool = multiprocessing.Pool(multiprocessing.cpu_count()) 
    i_list = range(8)
    pool.map(m1, i_list)       
    #利用map方法，将函数作用到表的每个元素上，这与built-in的map()函数类似，只是这里用5个进程并行处理。
    
输出：
0
1
4
9
16
25
36
49
```

#### 共享资源
* 在Linux进程间通信，可使用共享内存(shared memory)的原理
``` python
# modified from official documentation
import multiprocessing

def f(n, a):
    n.value   = 3.14
    a[0]      = 5
    
#在主进程的内存空间中创建共享的内存，即Value和Array两个对象
num   = multiprocessing.Value('d', 0.0)
arr   = multiprocessing.Array('i', range(10))


p = multiprocessing.Process(target=f, args=(num, arr))  #这里只有主进程和Process对象代表的进程
p.start()
p.join()

#在Process进程中，我们修改了Value和Array。回到主程序打印出结果，主程序也看到了2个对象改变，说明资源确实在两个进程间共享
print num.value
print arr[:]
```

#### Queue 方式的进程间通信
* Process之间肯定是需要通信的，OS提供了很多机制实现进程间通信。Python的multiprocessing包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据
```python
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    for value in ['A', 'B', 'C']:
        print 'Put %s to queue...' % value
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    while True:
        value = q.get(True)
        print 'Get %s from queue.' % value

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()

输出：
Put A to queue...
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.
```
* 在Unix/Linux下，multiprocessing封装了fork()调用使我们不需关注fork()细节。由于Windows没有fork调用，因此multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，若multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。

#### Manager
* Manager对象类似于服务器与客户之间的通信 (c/s)，与我们在Internet上的活动很类似。
* 我们用1个进程作为服务器，建立Manager来真正存放资源。其它的进程可通过参数传递或根据地址来访问Manager
* 建立连接后操作服务器上的资源。在防火墙允许的情况下我们完全可将Manager运用于多计算机从而模仿1个真实的网络情境。下面的例子中，我们对Manager的使用类似于shared memory，但可以共享更丰富的对象类型。
```python
import multiprocessing

def f(x, arr, l):
    x.value = 3.14
    arr[0] = 5
    l.append('Hello')

server = multiprocessing.Manager()
x    = server.Value('d', 0.0)
arr  = server.Array('i', range(10))
l    = server.list()

proc = multiprocessing.Process(target=f, args=(x, arr, l))
proc.start()
proc.join()

print(x.value)
print(arr)
print(l)
```
*   Manager利用list()方法提供了表的共享方式。实际上你可以利用**dict()**来共享词典，**Lock()**来共享threading.Lock()注意，我们共享的是threading.Lock，而不是进程的mutiprocessing.Lock。后者本身已经实现了进程共享)等。 这样Manager就允许我们共享更多样的对象。

参考：  
http://blog.csdn.net/a464057216/article/details/52735584
http://www.cnblogs.com/smallmars/p/7093603.html
http://blog.csdn.net/u014556057/article/details/61616902
http://www.cnblogs.com/vamei/archive/2012/10/13/2722254.html
